{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cf9b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "9d2916a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util functions\n",
    "\n",
    "def param(*dims) -> nn.Parameter:\n",
    "    data = torch.empty(dims)\n",
    "    nn.init.xavier_uniform_(data)\n",
    "    return nn.Parameter(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370832d5",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c75682",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int) -> None:\n",
    "        if d_model % num_heads != 0:\n",
    "            raise ValueError(\"Number d_model must be divisible by num_heads\")\n",
    "        super().__init__()\n",
    "        self.h = num_heads\n",
    "        self.d = d_model // num_heads\n",
    "        self.root_d = torch.sqrt(torch.tensor(self.d))\n",
    "\n",
    "        # trainable projections for all \"heads\"\n",
    "        # all in one param matrix.\n",
    "        self.wq = param(d_model, d_model)\n",
    "        self.wk = param(d_model, d_model)\n",
    "        self.wv = param(d_model, d_model)\n",
    "        # wo is needed for residual. Also in general it aligns the dimensions\n",
    "        # but in original paper dk = dv = d_model / h so it's not needed for that.\n",
    "        self.wo = param(d_model, d_model)\n",
    "        # FFN parametes\n",
    "        self.w1 = param(d_model, d_ff)\n",
    "        self.b1 = torch.zeros(d_ff)\n",
    "        self.w2 = param(d_ff, d_model)\n",
    "        self.b2 = torch.zeros(d_model)\n",
    "        # layer norms\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        mha = self.multi_head_attention(x, x, x)\n",
    "        x = self.ln1(x + mha)\n",
    "        ffn = torch.relu(x @ self.w1 + self.b1) @ self.w2 + self.b2\n",
    "        return self.ln2(x + ffn)\n",
    "    \n",
    "    def multi_head_attention(self, q: Tensor, k: Tensor, v: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Input shapes are (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # first project then split into \"heads\"\n",
    "        batch_size, seq_len, d_model = q.shape\n",
    "        reshape = lambda x: x.reshape(batch_size, seq_len, self.h, self.d).permute(0, 2, 1, 3)\n",
    "        q_proj = reshape(q @ self.wq)\n",
    "        k_proj = reshape(k @ self.wk)\n",
    "        v_proj = reshape(v @ self.wv)  # v is same dim as q, k\n",
    "        x = self.attention(q_proj, k_proj, v_proj)\n",
    "        # now we need to \"concat\" all the heads so we get d_model at the end\n",
    "        x = x.permute(0, 2, 1, 3).reshape(batch_size, seq_len, d_model)\n",
    "        # then do one more linear projection\n",
    "        return x @ self.wo\n",
    "    \n",
    "    def attention(self, q: Tensor, k: Tensor, v: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Input shapes for q and k are:\n",
    "            (batch_size, seq_len, num_heads, d_model // num_heads)\n",
    "        And for v it is:\n",
    "            (batch_size, num_heads, d_v) where d_v is d_model // num_heads in encoder.\n",
    "\n",
    "        It does all \"heads\" in one go, i.e. broadcast matrix multiplication.\n",
    "        Output is the shape\n",
    "            (batch_size, seq_len, num_heads, d_v)\n",
    "        \"\"\"\n",
    "        return torch.softmax(q @ k.permute(0, 1, 3, 2) / self.root_d, dim=3) @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "6016fdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_layers: int, \n",
    "            emb_dim: int,  # token embedding dim. Same as d_model\n",
    "            num_heads: int,\n",
    "            d_ff: int  # hidden dim of FFN\n",
    "            ) -> None:\n",
    "        super().__init__()\n",
    "        layers = [EncoderLayer(emb_dim, num_heads, d_ff) for _ in range(n_layers)]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        x has shape (batch_size, seq_len, emb_dim)\n",
    "        \"\"\"\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "6bafe215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 13, 512])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_dim = 512\n",
    "batch_size = 7\n",
    "seq_len = 13\n",
    "num_heads = 8\n",
    "dff = 2048\n",
    "n_layers = 11\n",
    "\n",
    "x = torch.rand(batch_size, seq_len, emb_dim)\n",
    "\n",
    "encoder = Encoder(n_layers, emb_dim, num_heads, dff)\n",
    "\n",
    "encoder_output = encoder(x)\n",
    "encoder_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e496cb",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "65c6ef4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            d_model: int,\n",
    "            num_heads: int,\n",
    "            d_ff: int,\n",
    "        ) -> None:\n",
    "        if d_model % num_heads != 0:\n",
    "            raise ValueError(\"Number d_model must be divisible by num_heads\")\n",
    "        super().__init__()\n",
    "        self.h = num_heads\n",
    "        self.d = d_model // num_heads\n",
    "        self.root_d = torch.sqrt(torch.tensor(self.d))\n",
    "        # attention params\n",
    "            # bottom attention (masked one)\n",
    "        self.wq1 = param(d_model, d_model)\n",
    "        self.wk1 = param(d_model, d_model)\n",
    "        self.wv1 = param(d_model, d_model)\n",
    "        self.wo1 = param(d_model, d_model)\n",
    "            # top attention (with encoder output, no mask)\n",
    "        self.wq2 = param(d_model, d_model)\n",
    "        self.wk2 = param(d_model, d_model)\n",
    "        self.wv2 = param(d_model, d_model)\n",
    "        self.wo2 = param(d_model, d_model)\n",
    "        # FFN parametes\n",
    "        self.w1 = param(d_model, d_ff)\n",
    "        self.b1 = torch.zeros(d_ff)\n",
    "        self.w2 = param(d_ff, d_model)\n",
    "        self.b2 = torch.zeros(d_model)\n",
    "        # layer norms\n",
    "        self.ln1 = nn.LayerNorm(d_model)  # bottom attention\n",
    "        self.ln2 = nn.LayerNorm(d_model)  # top attention\n",
    "        self.ln_ffn = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, encoder_output: Tensor, decoder_output: Tensor) -> Tensor:\n",
    "        # bottom blocks\n",
    "        mha1 = self.multi_head_attention(\n",
    "                decoder_output,\n",
    "                decoder_output,\n",
    "                decoder_output,\n",
    "                self.wq1,\n",
    "                self.wk1,\n",
    "                self.wv1,\n",
    "                self.wo1,\n",
    "                apply_mask=True\n",
    "            )\n",
    "        x = self.ln1(mha1 + decoder_output)\n",
    "        \n",
    "        # now add in the encoder signal\n",
    "        mha2 = self.multi_head_attention(\n",
    "                x,\n",
    "                encoder_output,\n",
    "                encoder_output,\n",
    "                self.wq2,\n",
    "                self.wk2,\n",
    "                self.wv2,\n",
    "                self.wo2,\n",
    "                apply_mask=False\n",
    "            )\n",
    "        x = self.ln2(mha2 + x)\n",
    "        \n",
    "        # now feed forward network\n",
    "        ffn = torch.relu(x @ self.w1 + self.b1) @ self.w2 + self.b2\n",
    "        return self.ln_ffn(ffn + x)\n",
    "\n",
    "\n",
    "    def multi_head_attention(\n",
    "            self,\n",
    "            q: Tensor,\n",
    "            k: Tensor,\n",
    "            v: Tensor,\n",
    "            wq: Tensor,\n",
    "            wk: Tensor,\n",
    "            wv: Tensor,\n",
    "            wo: Tensor,\n",
    "            apply_mask: bool,\n",
    "        ) -> Tensor:\n",
    "        \"\"\"\n",
    "        Input shapes are (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        batch_size, q_seq_len, d_model = q.shape\n",
    "        k_seq_len = k.shape[1]\n",
    "        # reshape function will give us (batch, h, seq, d_model/h)\n",
    "        # so we can do broadcast multiplication with that\n",
    "        reshape = lambda x, seq_len: x.reshape(batch_size, seq_len, self.h, self.d).permute(0, 2, 1, 3)\n",
    "        q_proj = reshape(q @ wq, q_seq_len)\n",
    "        k_proj = reshape(k @ wk, k_seq_len)\n",
    "        v_proj = reshape(v @ wv, k_seq_len)\n",
    "        x = self.attention(q_proj, k_proj, v_proj, apply_mask)\n",
    "        # now we need to permute back to (batch, seq, h, d_model/h) \n",
    "        # then \"concat\" all the heads so we get d_model at the end\n",
    "        x = x.permute(0, 2, 1, 3).reshape(batch_size, q_seq_len, d_model)\n",
    "        # then do one more linear projection\n",
    "        return x @ wo\n",
    "\n",
    "    def attention(self, q: Tensor, k: Tensor, v: Tensor, apply_mask: bool) -> Tensor:\n",
    "        \"\"\"\n",
    "        If param apply_max == True, it will add -inf to the upper triangular matrix of Q*K^T\n",
    "        before applying softmax (i.e. those values will be essentially disregarded in the softmax).\n",
    "        Input shapes for q and k are:\n",
    "            (batch_size, seq_len, num_heads, d_model // num_heads)\n",
    "        And for v it is:\n",
    "            (batch_size, num_heads, d_v) where d_v is d_model // num_heads in encoder.\n",
    "\n",
    "        It does all \"heads\" in one go, i.e. broadcast matrix multiplication.\n",
    "        Output is the shape\n",
    "            (batch_size, seq_len, num_heads, d_v)\n",
    "        \"\"\"\n",
    "        qkt = q @ k.permute(0, 1, 3, 2)\n",
    "        if apply_mask:\n",
    "            qkt += self.get_mask(q.shape[2])\n",
    "        return torch.softmax(qkt / self.root_d, dim=3) @ v\n",
    "    \n",
    "    def get_mask(self, seq_len: int) -> Tensor:\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len, dtype=torch.bool))\n",
    "        return torch.where(mask, 0.0, float('-inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "c551a424",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_layers: int, \n",
    "            emb_dim: int,  # token embedding dim. Same as d_model\n",
    "            num_heads: int,\n",
    "            d_ff: int,  # hidden dim of FFN\n",
    "            vocab_size: int\n",
    "            ) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(DecoderLayer(emb_dim, num_heads, d_ff) for _ in range(n_layers))\n",
    "        self.final_projection = nn.Sequential(\n",
    "            nn.Linear(emb_dim, vocab_size),\n",
    "            nn.Softmax(-1),            \n",
    "        )\n",
    "\n",
    "    def __call__(self, encoder_output: Tensor, decoder_output: Tensor) -> Tensor:\n",
    "        x = decoder_output\n",
    "        for layer in self.layers:\n",
    "            x = layer(encoder_output, x)\n",
    "        return self.final_projection(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "240bfc7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 1, 1000])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(batch_size, 1, emb_dim)\n",
    "\n",
    "vocab_size = 1000\n",
    "decoder = Decoder(n_layers, emb_dim, num_heads, dff, vocab_size)\n",
    "\n",
    "decoder_output = decoder(encoder_output, y)\n",
    "decoder_output.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
