{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34d9ef36",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ba95437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import Optional\n",
    "\n",
    "# Implementation of Logistic Regression from scratch\n",
    "class LogisticRegression:\n",
    "    def __init__(self, lr: float = 0.1, n_iter: int = 1000) -> None:\n",
    "        # lr: learning rate for gradient descent\n",
    "        # n_iter: number of iterations for training\n",
    "        self.lr: float = lr\n",
    "        self.n_iter: int = n_iter\n",
    "        # weights and bias will be learned during training\n",
    "        self.weights: np.ndarray = np.array([])\n",
    "        self.bias: float = 0.0\n",
    "\n",
    "    def sigmoid(self, z: np.ndarray) -> np.ndarray:\n",
    "        # Sigmoid activation function to map predictions to probabilities\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        # X: feature matrix, shape (n_samples, n_features)\n",
    "        # y: target vector, shape (n_samples,)\n",
    "        n_samples, n_features = X.shape\n",
    "        # Initialize weights and bias to zeros\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0.0\n",
    "\n",
    "        # Gradient descent for n_iter steps\n",
    "        for _ in range(self.n_iter):\n",
    "            # Linear model: Xw + b\n",
    "            logits = np.dot(X, self.weights) + self.bias\n",
    "            # Apply sigmoid to get probabilities\n",
    "            pred_scores = self.sigmoid(logits)\n",
    "\n",
    "            # Compute gradients\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (pred_scores - y))\n",
    "            db = (1 / n_samples) * np.sum(pred_scores - y)\n",
    "\n",
    "            # Update weights and bias\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "\n",
    "    def predict_probs(self, X: np.ndarray) -> np.ndarray:\n",
    "        # Returns predicted probabilities for input features X\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        return self.sigmoid(linear_model)\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        # Returns binary predictions (0 or 1) based on probability threshold 0.5\n",
    "        y_predicted_probs = self.predict_probs(X)\n",
    "        return (y_predicted_probs >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "# Example usage of the LogisticRegression class\n",
    "\n",
    "# Generate a simple synthetic dataset for binary classification\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an instance of LogisticRegression and train it\n",
    "clf = LogisticRegression(lr=0.1, n_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "preds = clf.predict(X_test)\n",
    "# Calculate and print the accuracy\n",
    "accuracy = np.mean(preds == y_test)\n",
    "print(f\"Test accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cbde43",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "632a6544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Test accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from typing import Any\n",
    "\n",
    "class KNNClassifier:\n",
    "    def __init__(self, k: int = 3) -> None:\n",
    "        self.k: int = k\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        X shape (n_samples, n_features)\n",
    "        y shape (n_samples,)\n",
    "        \"\"\"\n",
    "        self.X_train: np.ndarray = X\n",
    "        self.y_train: np.ndarray = y\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        X shape (n_samples, n_features)\n",
    "        Returns:\n",
    "            Predicted class labels for each sample in X. shape (n_samples,)\n",
    "        \"\"\"\n",
    "        predictions = [self._predict(x) for x in X]\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def _predict(self, x: np.ndarray) -> Any:\n",
    "        \"\"\"\n",
    "        x: np.ndarray, shape (n_features,)\n",
    "            Single sample feature vector.\n",
    "        Returns:\n",
    "            Predicted class label for the sample.\n",
    "        \"\"\"\n",
    "        distances = np.linalg.norm(self.X_train - x, axis=1)\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        k_nearest_labels = self.y_train[k_indices]\n",
    "        most_common = Counter(k_nearest_labels).most_common(1)\n",
    "        return most_common[0][0]\n",
    "\n",
    "# Example usage:\n",
    "knn = KNNClassifier(k=3)\n",
    "knn.fit(X_train, y_train)\n",
    "knn_preds = knn.predict(X_test)\n",
    "knn_accuracy = np.mean(knn_preds == y_test)\n",
    "print(f\"KNN Test accuracy: {knn_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd69273",
   "metadata": {},
   "source": [
    "### K means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75a2db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 cluster assignments: [2 1 0 3 1 3 0 2 0 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class KMeans:\n",
    "    def __init__(self, n_clusters: int = 2, n_iter: int = 100, random_state: int = 42) -> None:\n",
    "        self.n_clusters = n_clusters\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "        self.centroids: np.ndarray = np.ndarray([])\n",
    "\n",
    "    def fit(self, X: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        X shape (n_samples, n_features)\n",
    "        \"\"\"\n",
    "        np.random.seed(self.random_state)\n",
    "        n_samples, n_features = X.shape\n",
    "        # Randomly initialize centroids by selecting random samples\n",
    "        random_idxs = np.random.choice(n_samples, self.n_clusters, replace=False)\n",
    "        self.centroids = X[random_idxs]\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            # Assign each sample to the nearest centroid\n",
    "            labels = self.predict(X)\n",
    "            # Compute new centroids as mean of assigned points\n",
    "            new_centroids = []\n",
    "            for i in range(self.n_clusters):\n",
    "                # Get all points assigned to cluster i\n",
    "                cluster_points = X[labels == i]\n",
    "                if np.any(labels == i):\n",
    "                    # If there are points assigned, compute their mean\n",
    "                    centroid = cluster_points.mean(axis=0)\n",
    "                else:\n",
    "                    # If no points assigned, keep the old centroid\n",
    "                    centroid = self.centroids[i]\n",
    "                new_centroids.append(centroid)\n",
    "            new_centroids = np.array(new_centroids)\n",
    "            # If centroids do not change, break\n",
    "            if np.allclose(self.centroids, new_centroids):\n",
    "                break\n",
    "            self.centroids = new_centroids\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        X: np.ndarray, shape (n_samples, n_features)\n",
    "            Data to assign to clusters.\n",
    "            Assigns each sample to the nearest centroid.\n",
    "        Returns:\n",
    "            np.ndarray, shape (n_samples,)\n",
    "            Cluster index for each sample.\n",
    "        \"\"\"\n",
    "        distances = np.linalg.norm(X[:, np.newaxis] - self.centroids, axis=2)\n",
    "        return np.argmin(distances, axis=1)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "kmeans = KMeans(n_clusters=4, n_iter=100, random_state=42)\n",
    "kmeans.fit(X)\n",
    "cluster_labels = kmeans.predict(X)\n",
    "print(\"First 10 cluster assignments:\", cluster_labels[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
